{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\") \n",
    "train_data = train_data.loc[train_data.airline_sentiment.dropna().index]\n",
    "train_data = train_data.loc[train_data.text.dropna().index]\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\") \n",
    "test_data = test_data.loc[test_data.text.dropna().index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569682010270101504</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zsalim03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir In car gng to DFW. Pulled over 1h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 18:15:50 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569608307184242688</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sa_craig</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir after all, the plane didn’t land ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 13:22:57 -0800</td>\n",
       "      <td>College Station, TX</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>567879304593408001</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DanaChristos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>@SouthwestAir can't believe how many paying cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 18:52:31 -0800</td>\n",
       "      <td>CT</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569757651539660801</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rossj987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways I can legitimately say that I would...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 23:16:24 -0800</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>569900705852608513</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tranpham18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir still no response from AA. great ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 08:44:51 -0800</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>570304244001193984</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anthony_Scerri</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways Been stuck for 40+ minutes due to l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:28:22 -0800</td>\n",
       "      <td>Astoria, NY</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>567847737061941249</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mttdprkr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways 4 hours... 4 hours... FOUR HOURS.  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 16:47:05 -0800</td>\n",
       "      <td>Vancouver, WA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>567823564167192576</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>miaerolinea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice RT @VirginAmerica: The man of steel might...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 15:11:02 -0800</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Caracas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>570273819287531520</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GoldensPleasure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Aww Thanks AA..DFW was on GMA up ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 09:27:28 -0800</td>\n",
       "      <td>East Coast     CT.</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>569341769114128386</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suntoshi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united the lounge tells us they have no pillo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 19:43:50 -0800</td>\n",
       "      <td>Bedford, Nh</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3660 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id         airline airline_sentiment_gold  \\\n",
       "0     569682010270101504        American                    NaN   \n",
       "1     569608307184242688        American                    NaN   \n",
       "2     567879304593408001       Southwest                    NaN   \n",
       "3     569757651539660801      US Airways                    NaN   \n",
       "4     569900705852608513        American                    NaN   \n",
       "...                  ...             ...                    ...   \n",
       "3655  570304244001193984      US Airways                    NaN   \n",
       "3656  567847737061941249      US Airways                    NaN   \n",
       "3657  567823564167192576  Virgin America                    NaN   \n",
       "3658  570273819287531520        American                    NaN   \n",
       "3659  569341769114128386          United                    NaN   \n",
       "\n",
       "                 name negativereason_gold  retweet_count  \\\n",
       "0            zsalim03                 NaN              0   \n",
       "1            sa_craig                 NaN              0   \n",
       "2        DanaChristos                 NaN              1   \n",
       "3            rossj987                 NaN              0   \n",
       "4          tranpham18                 NaN              0   \n",
       "...               ...                 ...            ...   \n",
       "3655   Anthony_Scerri                 NaN              0   \n",
       "3656         mttdprkr                 NaN              0   \n",
       "3657      miaerolinea                 NaN              1   \n",
       "3658  GoldensPleasure                 NaN              0   \n",
       "3659         suntoshi                 NaN              0   \n",
       "\n",
       "                                                   text tweet_coord  \\\n",
       "0     @AmericanAir In car gng to DFW. Pulled over 1h...         NaN   \n",
       "1     @AmericanAir after all, the plane didn’t land ...         NaN   \n",
       "2     @SouthwestAir can't believe how many paying cu...         NaN   \n",
       "3     @USAirways I can legitimately say that I would...         NaN   \n",
       "4     @AmericanAir still no response from AA. great ...         NaN   \n",
       "...                                                 ...         ...   \n",
       "3655  @USAirways Been stuck for 40+ minutes due to l...         NaN   \n",
       "3656  @USAirways 4 hours... 4 hours... FOUR HOURS.  ...         NaN   \n",
       "3657  Nice RT @VirginAmerica: The man of steel might...         NaN   \n",
       "3658  @AmericanAir Aww Thanks AA..DFW was on GMA up ...         NaN   \n",
       "3659  @united the lounge tells us they have no pillo...         NaN   \n",
       "\n",
       "                  tweet_created       tweet_location  \\\n",
       "0     2015-02-22 18:15:50 -0800                Texas   \n",
       "1     2015-02-22 13:22:57 -0800  College Station, TX   \n",
       "2     2015-02-17 18:52:31 -0800                   CT   \n",
       "3     2015-02-22 23:16:24 -0800     Washington, D.C.   \n",
       "4     2015-02-23 08:44:51 -0800        New York City   \n",
       "...                         ...                  ...   \n",
       "3655  2015-02-24 11:28:22 -0800          Astoria, NY   \n",
       "3656  2015-02-17 16:47:05 -0800        Vancouver, WA   \n",
       "3657  2015-02-17 15:11:02 -0800            Worldwide   \n",
       "3658  2015-02-24 09:27:28 -0800   East Coast     CT.   \n",
       "3659  2015-02-21 19:43:50 -0800          Bedford, Nh   \n",
       "\n",
       "                   user_timezone  \n",
       "0     Central Time (US & Canada)  \n",
       "1     Central Time (US & Canada)  \n",
       "2     Eastern Time (US & Canada)  \n",
       "3     Eastern Time (US & Canada)  \n",
       "4     Eastern Time (US & Canada)  \n",
       "...                          ...  \n",
       "3655                       Quito  \n",
       "3656  Pacific Time (US & Canada)  \n",
       "3657                     Caracas  \n",
       "3658  Central Time (US & Canada)  \n",
       "3659  Eastern Time (US & Canada)  \n",
       "\n",
       "[3660 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def getStopWords():\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop_words += punctuation\n",
    "    stop_words = set(stop_words)\n",
    "#     f = open(\"stop_words_english.txt\", \"r\")         # opening the file in which more stop words are present \n",
    "#     a=f.read()      \n",
    "#     s=a.split()               \n",
    "#     stop_words.update(s) \n",
    "    return stop_words\n",
    "\n",
    "def lemmatizer_pos_tag(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def cleaning_doc(text):\n",
    "    # remove stop words1\n",
    "    a=1\n",
    "    output = []\n",
    "    words_arr = word_tokenize(text)\n",
    "    for w in words_arr:\n",
    "        if w.lower() not in stop_words:\n",
    "            pos = pos_tag([w])\n",
    "            pos = lemmatizer_pos_tag(pos[0][1])\n",
    "            final_w = lemmatizer.lemmatize(w, pos)\n",
    "            output.append(final_w)\n",
    "    return output\n",
    "\n",
    "stop_words = getStopWords()\n",
    "\n",
    "train_docs = []\n",
    "train_categories = []\n",
    "for i in range(train_data.shape[0]):\n",
    "    text = train_data.iloc[i,7]\n",
    "    train_docs.append(cleaning_doc(text))\n",
    "    train_categories.append(train_data.iloc[i,1])\n",
    "\n",
    "test_docs = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    text = test_data.iloc[i,6]\n",
    "    test_docs.append(cleaning_doc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SouthwestAir', 'schedule', 'morning', '2', 'day', 'fact', 'yes', '..', 'sure', 'even', 'flight', 'one', 'Cancelled', 'Flightled'] negative\n",
      "['AmericanAir', 'car', 'gng', 'DFW', 'Pulled', '1hr', 'ago', 'icy', 'road', 'On-hold', 'AA', 'since', '1hr', 'Ca', \"n't\", 'reach', 'arpt', 'AA2450', 'Wat', '2']\n"
     ]
    }
   ],
   "source": [
    "print(train_docs[0],train_categories[0])\n",
    "print(test_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_docs = [\" \".join(d) for d in train_docs]\n",
    "text_test_docs = [\" \".join(d) for d in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SouthwestAir schedule morning 2 day fact yes .. sure even flight one Cancelled Flightled\n",
      "AmericanAir car gng DFW Pulled 1hr ago icy road On-hold AA since 1hr Ca n't reach arpt AA2450 Wat 2\n"
     ]
    }
   ],
   "source": [
    "print(text_train_docs[0])\n",
    "print(text_test_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features = 4000)\n",
    "x_train_features = count_vectorizer.fit_transform(text_train_docs)\n",
    "x_train_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0016',\n",
       " '00pm',\n",
       " '02',\n",
       " '03',\n",
       " '05',\n",
       " '05pm',\n",
       " '08',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1007',\n",
       " '105',\n",
       " '1051',\n",
       " '1071',\n",
       " '1080',\n",
       " '1098',\n",
       " '10a',\n",
       " '10am',\n",
       " '10hrs',\n",
       " '10pm',\n",
       " '11',\n",
       " '117',\n",
       " '1171',\n",
       " '11am',\n",
       " '11pm',\n",
       " '11th',\n",
       " '12',\n",
       " '1230',\n",
       " '12am',\n",
       " '13',\n",
       " '130',\n",
       " '1357',\n",
       " '1359',\n",
       " '136',\n",
       " '1388',\n",
       " '1389',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '1472',\n",
       " '1491',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1514',\n",
       " '152',\n",
       " '1533',\n",
       " '1562',\n",
       " '1583',\n",
       " '15minutes',\n",
       " '15th',\n",
       " '16',\n",
       " '1625',\n",
       " '1679',\n",
       " '17',\n",
       " '1701',\n",
       " '1706',\n",
       " '1750',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1826',\n",
       " '1861',\n",
       " '1898',\n",
       " '19',\n",
       " '1917',\n",
       " '195',\n",
       " '1970',\n",
       " '1971',\n",
       " '1am',\n",
       " '1hr',\n",
       " '1k',\n",
       " '1pm',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2012',\n",
       " '2014',\n",
       " '2015',\n",
       " '2034',\n",
       " '20min',\n",
       " '20pm',\n",
       " '21',\n",
       " '21st',\n",
       " '22',\n",
       " '2258',\n",
       " '23',\n",
       " '2324',\n",
       " '23rd',\n",
       " '24',\n",
       " '2470',\n",
       " '24h',\n",
       " '24hr',\n",
       " '24hrs',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '26',\n",
       " '2692',\n",
       " '27',\n",
       " '275',\n",
       " '27l',\n",
       " '28',\n",
       " '28th',\n",
       " '29',\n",
       " '2955',\n",
       " '2990176298',\n",
       " '2b',\n",
       " '2d',\n",
       " '2day',\n",
       " '2days',\n",
       " '2hours',\n",
       " '2hr',\n",
       " '2hrs',\n",
       " '2littlebirds',\n",
       " '2morrow',\n",
       " '2nd',\n",
       " '2nite',\n",
       " '2pm',\n",
       " '2x',\n",
       " '2xdaily',\n",
       " '30',\n",
       " '300',\n",
       " '3001885409',\n",
       " '3056',\n",
       " '30a',\n",
       " '30am',\n",
       " '30k',\n",
       " '30min',\n",
       " '30mins',\n",
       " '30pm',\n",
       " '30th',\n",
       " '31',\n",
       " '310',\n",
       " '3130',\n",
       " '32',\n",
       " '33',\n",
       " '336',\n",
       " '35',\n",
       " '350',\n",
       " '35pm',\n",
       " '36',\n",
       " '37',\n",
       " '382',\n",
       " '386',\n",
       " '39',\n",
       " '3am',\n",
       " '3d',\n",
       " '3fq3xelbon',\n",
       " '3hrs',\n",
       " '3pm',\n",
       " '3rd',\n",
       " '3thparty',\n",
       " '3x',\n",
       " '3yr',\n",
       " '40',\n",
       " '400',\n",
       " '400er',\n",
       " '403',\n",
       " '404',\n",
       " '40mins',\n",
       " '40pm',\n",
       " '40th',\n",
       " '413',\n",
       " '41g',\n",
       " '42',\n",
       " '428',\n",
       " '4322',\n",
       " '433',\n",
       " '435',\n",
       " '44',\n",
       " '440',\n",
       " '4420',\n",
       " '4435',\n",
       " '4438',\n",
       " '445',\n",
       " '45',\n",
       " '450',\n",
       " '4524',\n",
       " '4567',\n",
       " '45am',\n",
       " '45min',\n",
       " '45mins',\n",
       " '4649',\n",
       " '47',\n",
       " '475',\n",
       " '48',\n",
       " '494',\n",
       " '4am',\n",
       " '4hrs',\n",
       " '4ojrsdwpkk',\n",
       " '4pm',\n",
       " '4th',\n",
       " '4x',\n",
       " '50',\n",
       " '500',\n",
       " '501',\n",
       " '5080',\n",
       " '50am',\n",
       " '50k',\n",
       " '50pm',\n",
       " '51',\n",
       " '53',\n",
       " '5350',\n",
       " '55',\n",
       " '5534',\n",
       " '556',\n",
       " '55am',\n",
       " '55pm',\n",
       " '58',\n",
       " '59',\n",
       " '597',\n",
       " '599',\n",
       " '5am',\n",
       " '5hrs',\n",
       " '5pm',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '606',\n",
       " '62',\n",
       " '623',\n",
       " '630',\n",
       " '636',\n",
       " '639',\n",
       " '645',\n",
       " '64gb',\n",
       " '65',\n",
       " '653',\n",
       " '654',\n",
       " '683',\n",
       " '686',\n",
       " '699',\n",
       " '6am',\n",
       " '6hrs',\n",
       " '6pm',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '703',\n",
       " '719',\n",
       " '72',\n",
       " '723',\n",
       " '728',\n",
       " '72rmpkogwu',\n",
       " '73',\n",
       " '7300',\n",
       " '737',\n",
       " '746',\n",
       " '747',\n",
       " '75',\n",
       " '750',\n",
       " '757',\n",
       " '767',\n",
       " '777',\n",
       " '787',\n",
       " '79',\n",
       " '795',\n",
       " '799',\n",
       " '7am',\n",
       " '7d',\n",
       " '7pm',\n",
       " '7t1rdrcre6',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '80th',\n",
       " '810',\n",
       " '830',\n",
       " '834',\n",
       " '85832',\n",
       " '894',\n",
       " '8am',\n",
       " '8aug',\n",
       " '8b',\n",
       " '8h',\n",
       " '8hr',\n",
       " '8pm',\n",
       " '8wbzorrn3c',\n",
       " '90',\n",
       " '917',\n",
       " '919',\n",
       " '953',\n",
       " '989',\n",
       " '99',\n",
       " '9am',\n",
       " '9pm',\n",
       " 'a1',\n",
       " 'a320',\n",
       " 'aa',\n",
       " 'aa2444',\n",
       " 'aa76',\n",
       " 'aa953',\n",
       " 'aadvantage',\n",
       " 'aafail',\n",
       " 'abandon',\n",
       " 'abc',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'abq',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absurd',\n",
       " 'abt',\n",
       " 'abuse',\n",
       " 'abysmal',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accommodate',\n",
       " 'accommodation',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'acct',\n",
       " 'accts',\n",
       " 'accurate',\n",
       " 'accuse',\n",
       " 'acknowledge',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adds',\n",
       " 'addtl',\n",
       " 'adjacent',\n",
       " 'adjustment',\n",
       " 'admiral',\n",
       " 'admirals',\n",
       " 'admit',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advertise',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advisory',\n",
       " 'aerojobmarket',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affiliate',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'aggravate',\n",
       " 'aggressive',\n",
       " 'agnt',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agt',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahhhh',\n",
       " 'ahold',\n",
       " 'ai',\n",
       " 'air',\n",
       " 'airbus',\n",
       " 'aircanada',\n",
       " 'aircraft',\n",
       " 'airfare',\n",
       " 'airline',\n",
       " 'airlinegeeks',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airway',\n",
       " 'airways',\n",
       " 'aisle',\n",
       " 'aka',\n",
       " 'alaska',\n",
       " 'albany',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'ali',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allende',\n",
       " 'allergy',\n",
       " 'alliance',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'altitude',\n",
       " 'always',\n",
       " 'alwayslate',\n",
       " 'am',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanair',\n",
       " 'americanairlines',\n",
       " 'americanview',\n",
       " 'amex',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amybruni',\n",
       " 'ana',\n",
       " 'anamarketers',\n",
       " 'and',\n",
       " 'andrew',\n",
       " 'andrew_wasila',\n",
       " 'andrews',\n",
       " 'aneqxzr4bp',\n",
       " 'angel',\n",
       " 'angry',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annricord',\n",
       " 'annual',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anthony',\n",
       " 'anticipate',\n",
       " 'antonio',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'apologize',\n",
       " 'apologizes',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'appalled',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appease',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'approx',\n",
       " 'apps',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'aqjn4hwnac',\n",
       " 'ardent',\n",
       " 'are',\n",
       " 'area',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'arizona',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'arrangement',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'article',\n",
       " 'aruba',\n",
       " 'as',\n",
       " 'asap',\n",
       " 'ase',\n",
       " 'ashamed',\n",
       " 'asia',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'askpaypal',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspen',\n",
       " 'assault',\n",
       " 'asset',\n",
       " 'asshole',\n",
       " 'assign',\n",
       " 'assignment',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'associate',\n",
       " 'assult',\n",
       " 'assume',\n",
       " 'assure',\n",
       " 'astound',\n",
       " 'at',\n",
       " 'atc',\n",
       " 'athlete',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atlantic',\n",
       " 'atleast',\n",
       " 'atrocious',\n",
       " 'attach',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attendant',\n",
       " 'attendee',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'auh',\n",
       " 'aus',\n",
       " 'austin',\n",
       " 'austinairport',\n",
       " 'australia',\n",
       " 'auto',\n",
       " 'automate',\n",
       " 'automatically',\n",
       " 'autoresponse',\n",
       " 'avail',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avgeek',\n",
       " 'aviation',\n",
       " 'avis',\n",
       " 'aviv',\n",
       " 'avoid',\n",
       " 'await',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'b1',\n",
       " 'b11',\n",
       " 'b12',\n",
       " 'b16',\n",
       " 'b4',\n",
       " 'b6',\n",
       " 'b737',\n",
       " 'b767',\n",
       " 'b787',\n",
       " 'ba',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'backlog',\n",
       " 'backup',\n",
       " 'bad',\n",
       " 'badcustomerservice',\n",
       " 'badge',\n",
       " 'badly',\n",
       " 'badmgmt',\n",
       " 'badservice',\n",
       " 'bae',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'baggagelost',\n",
       " 'bags',\n",
       " 'bahamas',\n",
       " 'bait',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'baltimore',\n",
       " 'band',\n",
       " 'bank',\n",
       " 'bankrupt',\n",
       " 'bar',\n",
       " 'barbados',\n",
       " 'barbara',\n",
       " 'barely',\n",
       " 'base',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'bathroom',\n",
       " 'battery',\n",
       " 'battierccipuppy',\n",
       " 'batting',\n",
       " 'battle',\n",
       " 'bay',\n",
       " 'bc',\n",
       " 'bday',\n",
       " 'bdl',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beanie',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'beer',\n",
       " 'before',\n",
       " 'begin',\n",
       " 'behalf',\n",
       " 'behavior',\n",
       " 'behind',\n",
       " 'belfast',\n",
       " 'believe',\n",
       " 'belize',\n",
       " 'belong',\n",
       " 'belonging',\n",
       " 'belt',\n",
       " 'benefit',\n",
       " 'bereavement',\n",
       " 'bergstrom',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'bestairlineever',\n",
       " 'bestemployees',\n",
       " 'bestflightever',\n",
       " 'besty',\n",
       " 'bet',\n",
       " 'betsy',\n",
       " 'better',\n",
       " 'beverage',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bf',\n",
       " 'bff',\n",
       " 'bgm',\n",
       " 'bgr',\n",
       " 'bhm',\n",
       " 'bicycle',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'bin',\n",
       " 'bio',\n",
       " 'bird',\n",
       " 'birmingham',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'biz',\n",
       " 'bk',\n",
       " 'black',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'blasting',\n",
       " 'blatant',\n",
       " 'bless',\n",
       " 'blew',\n",
       " 'block',\n",
       " 'blog',\n",
       " 'bloody',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'bluemanity',\n",
       " 'bna',\n",
       " 'board',\n",
       " 'boarded',\n",
       " 'boarding',\n",
       " 'boat',\n",
       " 'boeing',\n",
       " 'bogota',\n",
       " 'boise',\n",
       " 'bom',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'booking',\n",
       " 'boom',\n",
       " 'boot',\n",
       " 'booze',\n",
       " 'bora',\n",
       " 'border',\n",
       " 'bos',\n",
       " 'boston',\n",
       " 'bostonlogan',\n",
       " 'bot',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bougth',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'brag',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brandloveaffair',\n",
       " 'brandmance',\n",
       " 'brave',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'bridge',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'british_airways',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brokenpromises',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'browser',\n",
       " 'bruh',\n",
       " 'brushing',\n",
       " 'brutal',\n",
       " 'bs',\n",
       " 'bttr',\n",
       " 'btw',\n",
       " 'buck',\n",
       " 'buddy',\n",
       " 'buenos',\n",
       " 'buf',\n",
       " 'buffalo',\n",
       " 'bug',\n",
       " 'bull',\n",
       " 'bullshit',\n",
       " 'bum',\n",
       " 'bummer',\n",
       " 'bump',\n",
       " 'bumped',\n",
       " 'bumping',\n",
       " 'bunch',\n",
       " 'burning',\n",
       " 'burningman',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'bussey',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'bwi',\n",
       " 'bwi_airport',\n",
       " 'c19',\n",
       " 'c26',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cabin',\n",
       " 'cabo',\n",
       " 'cache',\n",
       " 'cake',\n",
       " 'cal',\n",
       " 'calendar',\n",
       " 'calgary',\n",
       " 'cali',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callback',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'camera',\n",
       " 'can',\n",
       " 'cana',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'cancelled',\n",
       " 'cancun',\n",
       " 'cant',\n",
       " 'capa_aviation',\n",
       " 'capable',\n",
       " 'capacity',\n",
       " 'capital',\n",
       " 'capt',\n",
       " 'captain',\n",
       " 'captive',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'cargo',\n",
       " 'carousel',\n",
       " 'carpet',\n",
       " 'carrie',\n",
       " 'carrier',\n",
       " 'carrieunderwood',\n",
       " 'carry',\n",
       " 'carryon',\n",
       " 'carryons',\n",
       " 'carseat',\n",
       " 'cart',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'cater',\n",
       " 'catering',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'cb',\n",
       " 'cc',\n",
       " 'celebrate',\n",
       " 'cell',\n",
       " 'cellphone',\n",
       " 'center',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'century',\n",
       " 'ceo',\n",
       " 'cert',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certificate',\n",
       " 'cessna',\n",
       " 'chair',\n",
       " 'chairman',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changes',\n",
       " 'channel',\n",
       " 'chaos',\n",
       " 'char',\n",
       " 'character',\n",
       " 'charge',\n",
       " 'charity',\n",
       " 'charleston',\n",
       " 'charlotte',\n",
       " 'charm',\n",
       " 'charter',\n",
       " 'chase',\n",
       " 'chasefoster',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'cheapflights',\n",
       " 'cheat',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checkin',\n",
       " 'checkout',\n",
       " 'checkpoint',\n",
       " 'cheer',\n",
       " 'cheers',\n",
       " 'cheese',\n",
       " 'chef',\n",
       " 'cherry',\n",
       " 'chi',\n",
       " 'chicago',\n",
       " 'chicagotribune',\n",
       " 'chicken',\n",
       " 'child',\n",
       " 'chill',\n",
       " 'china',\n",
       " 'chip',\n",
       " 'cho',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chose',\n",
       " 'christmas',\n",
       " 'chrome',\n",
       " 'chronicleherald',\n",
       " 'cincy',\n",
       " 'circle',\n",
       " 'circumstance',\n",
       " 'city',\n",
       " 'claim',\n",
       " 'claimed',\n",
       " 'clarification',\n",
       " 'clarify',\n",
       " 'class',\n",
       " 'cldnt',\n",
       " 'cle',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'cleveland',\n",
       " 'click',\n",
       " 'clicked',\n",
       " 'client',\n",
       " 'close',\n",
       " 'closer',\n",
       " 'closest',\n",
       " 'closing',\n",
       " 'clothes',\n",
       " 'clothing',\n",
       " 'cloud',\n",
       " 'clown',\n",
       " 'clt',\n",
       " 'club',\n",
       " 'clue',\n",
       " 'cluster',\n",
       " 'cmh',\n",
       " 'cmon',\n",
       " 'cnn',\n",
       " 'cnnbrk',\n",
       " 'cnxn',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'coast',\n",
       " 'coat',\n",
       " 'cockpit',\n",
       " 'cocktail',\n",
       " 'code',\n",
       " 'codeshare',\n",
       " 'coffee',\n",
       " 'coincidence',\n",
       " 'cold',\n",
       " 'colleague',\n",
       " 'college',\n",
       " 'colo',\n",
       " 'color',\n",
       " 'columbia',\n",
       " 'columbus',\n",
       " 'com',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'comcast',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comfortably',\n",
       " 'comm',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'commercial',\n",
       " 'commit',\n",
       " 'commitment',\n",
       " 'common',\n",
       " 'comms',\n",
       " 'communicate',\n",
       " 'communication',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'companion',\n",
       " 'company',\n",
       " 'compare',\n",
       " 'compassion',\n",
       " 'comped',\n",
       " 'compensate',\n",
       " 'compensation',\n",
       " 'competition',\n",
       " 'competitor',\n",
       " 'complain',\n",
       " 'complains',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complicate',\n",
       " 'compliment',\n",
       " 'complimentary',\n",
       " 'compound',\n",
       " 'computer',\n",
       " 'con',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_features = count_vectorizer.transform(text_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3660x4000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33229 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier 1 : 0.771\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(C=1.5)\n",
    "svc.fit(x_train_features, train_categories)\n",
    "a = svc.predict(x_test_features)\n",
    "b = pd.Series(a)\n",
    "b.to_csv(\"prediction.csv\",header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9092896174863389"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(x_train_features, train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99408014571949"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.7508\n",
    "from sklearn.ensemble import RandomForestClassifier as SVC\n",
    "svc = SVC()\n",
    "svc.fit(x_train_features, train_categories)\n",
    "a = svc.predict(x_test_features)\n",
    "b = pd.Series(a)\n",
    "b.to_csv(\"prediction1.csv\",header=False, index=False)\n",
    "svc.score(x_train_features, train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8306010928961749"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.7667\n",
    "from sklearn.naive_bayes import MultinomialNB as SVC\n",
    "svc = SVC(alpha=0.0001)\n",
    "svc.fit(x_train_features, train_categories)\n",
    "a = svc.predict(x_test_features)\n",
    "b = pd.Series(a)\n",
    "b.to_csv(\"prediction2.csv\",header=False, index=False)\n",
    "svc.score(x_train_features, train_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "max_df = 25 means \"ignore terms that appear in more than 25 documents\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vectorizer1 = TfidfVectorizer(max_features = 5000, ngram_range = (1,2), max_df=0.8, min_df=0.0001)\n",
    "x_train_features = count_vectorizer1.fit_transform(text_train_docs)\n",
    "x_train_features.todense()\n",
    "# text_train_docs[0]\n",
    "x_test_features = count_vectorizer1.transform(text_test_docs)\n",
    "len(count_vectorizer1.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9817850637522769"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(C=1.5)\n",
    "svc.fit(x_train_features, train_categories)\n",
    "a = svc.predict(x_test_features)\n",
    "b = pd.Series(a)\n",
    "b.to_csv(\"prediction3.csv\",header=False, index=False)\n",
    "count_vectorizer.get_feature_names()\n",
    "svc.score(x_train_features, train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936247723132969"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.7508\n",
    "from sklearn.ensemble import RandomForestClassifier as SVC\n",
    "svc = SVC(n_estimators=2000, n_jobs=-1)\n",
    "svc.fit(x_train_features, train_categories)\n",
    "a = svc.predict(x_test_features)\n",
    "b = pd.Series(a)\n",
    "b.to_csv(\"prediction4.csv\",header=False, index=False)\n",
    "svc.score(x_train_features, train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8586520947176685"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.7667\n",
    "from sklearn.naive_bayes import MultinomialNB as SVC\n",
    "svc = SVC(alpha=0.0001)\n",
    "svc.fit(x_train_features, train_categories)\n",
    "a = svc.predict(x_test_features)\n",
    "b = pd.Series(a)\n",
    "b.to_csv(\"prediction5.csv\",header=False, index=False)\n",
    "svc.score(x_train_features, train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
